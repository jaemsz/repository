{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "schema = StructType().add(\"class\", StringType(), True).add(\"message\", StringType(), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.schema(schema).csv(\"spam.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(df.message.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|             message|\n",
      "+-----+--------------------+\n",
      "|  0.0|Go until jurong p...|\n",
      "|  0.0|Ok lar... Joking ...|\n",
      "|  1.0|Free entry in 2 a...|\n",
      "|  0.0|U dun say so earl...|\n",
      "|  0.0|Nah I don't think...|\n",
      "|  1.0|FreeMsg Hey there...|\n",
      "|  0.0|Even my brother i...|\n",
      "|  0.0|As per your reque...|\n",
      "|  1.0|WINNER!! As a val...|\n",
      "|  1.0|Had your mobile 1...|\n",
      "|  0.0|I'm gonna be home...|\n",
      "|  1.0|SIX chances to wi...|\n",
      "|  1.0|URGENT! You have ...|\n",
      "|  0.0|I've been searchi...|\n",
      "|  0.0|I HAVE A DATE ON ...|\n",
      "|  1.0|XXXMobileMovieClu...|\n",
      "|  0.0|Oh k...i'm watchi...|\n",
      "|  0.0|Eh u remember how...|\n",
      "|  0.0|Fine if that��s t...|\n",
      "|  1.0|England v Macedon...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"class\", outputCol=\"label\").setHandleInvalid(\"keep\")\n",
    "indexed = indexer.fit(df).transform(df)\n",
    "indexed = indexed.select(f.col(\"label\"), f.col(\"message\"))\n",
    "indexed = indexed.filter(indexed.label != 2.0)\n",
    "indexed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "replaced = indexed.select(f.col(\"label\"), f.col(\"message\"), \n",
    "                          f.regexp_replace(f.col(\"message\"), \"[,.\\-\\!\\?\\$]\", \"\").alias(\"replaced\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"replaced\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(replaced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "filteredData = remover.transform(wordsData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = filteredData.select(f.col(\"label\"), f.col(\"filtered\"))\n",
    "training, test = dataset.randomSplit([0.8, 0.2], seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/32231049/how-to-use-spark-naive-bayes-classifier-for-text-classification-with-idf\n",
    "\n",
    "from pyspark.mllib.feature import HashingTF, IDF\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import NaiveBayes   \n",
    "\n",
    "# Split data into labels and features, transform\n",
    "# preservesPartitioning is not really required\n",
    "# since map without partitioner shouldn't trigger repartitiong\n",
    "labels = training.rdd.map(\n",
    "    lambda doc: doc[\"label\"],  # Standard Python dict access \n",
    "    preservesPartitioning=True # This is obsolete.\n",
    ")\n",
    "\n",
    "tf = HashingTF(numFeatures=20000).transform( ## Use much larger number in practice\n",
    "    training.rdd.map(lambda doc: doc[\"filtered\"], \n",
    "    preservesPartitioning=True))\n",
    "\n",
    "idf = IDF().fit(tf)\n",
    "tfidf = idf.transform(tf)\n",
    "\n",
    "# Combine using zip\n",
    "training = labels.zip(tfidf).map(lambda x: LabeledPoint(x[0], x[1]))\n",
    "\n",
    "# Train and check\n",
    "model = NaiveBayes.train(training)\n",
    "labels_and_preds = labels.zip(model.predict(tfidf)).map(\n",
    "    lambda x: {\"actual\": x[0], \"predicted\": float(x[1])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9860611510791367"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from operator import itemgetter\n",
    "\n",
    "metrics = MulticlassMetrics(\n",
    "    labels_and_preds.map(itemgetter(\"actual\", \"predicted\")))\n",
    "\n",
    "metrics.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = test.rdd.map(\n",
    "    lambda doc: doc[\"label\"],  # Standard Python dict access \n",
    "    preservesPartitioning=True # This is obsolete.\n",
    ")\n",
    "\n",
    "tf = HashingTF(numFeatures=20000).transform( ## Use much larger number in practice\n",
    "    test.rdd.map(lambda doc: doc[\"filtered\"], \n",
    "    preservesPartitioning=True))\n",
    "\n",
    "idf = IDF().fit(tf)\n",
    "tfidf = idf.transform(tf)\n",
    "\n",
    "# Combine using zip\n",
    "training = labels.zip(tfidf).map(lambda x: LabeledPoint(x[0], x[1]))\n",
    "\n",
    "# Train and check\n",
    "labels_and_preds = labels.zip(model.predict(tfidf)).map(\n",
    "    lambda x: {\"actual\": x[0], \"predicted\": float(x[1])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9741992882562278"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = MulticlassMetrics(\n",
    "    labels_and_preds.map(itemgetter(\"actual\", \"predicted\")))\n",
    "\n",
    "metrics.accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
